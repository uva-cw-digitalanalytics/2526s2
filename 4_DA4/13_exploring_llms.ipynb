{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Generative LLMs\n",
    "\n",
    "In this notebook, we will learn how to interact with open‑source generative language models using [ollama](https://github.com/ollama/ollama-python). We will cover everything from installing dependencies to downloading a model (for example, [gemma3](https://ollama.com/library/gemma3:270m)), generating text with various parameters, and even using the model to classify text in a pandas DataFrame.\n",
    "\n",
    "**Why does this matter?**\n",
    "\n",
    "Communication science has long studied how messages are crafted and interpreted. Similarly, generative models \"craft\" text based on patterns learned from human language. By experimenting with these models, we will gain insights into language construction, persuasion techniques, and narrative formation — a topic central to communication studies. This also sets upa nice context for us to start understanding machine learning.\n",
    "\n",
    "**What we will learn:**\n",
    "\n",
    "- How to set up our environment\n",
    "- How to download and use a model\n",
    "- How to generate text using simple and tuned prompts\n",
    "- How these technical concepts relate to real‑world communication principles\n",
    "- How to use open‑source LLMs for simple classification tasks on pandas data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "Before we start, ensure you have [ollama](https://ollama.com/download) installed on your machine. Follow the installation instructions on their website for your specific operating system.\n",
    "\n",
    "**Step 1: Install the Required Packages**\n",
    "\n",
    "We'll begin by installing the `ollama` package. This package provides the Python interface to work with generative language models via ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (0.5.4)\n",
      "Requirement already satisfied: httpx>=0.27 in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from ollama) (2.11.9)\n",
      "Requirement already satisfied: anyio in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from httpx>=0.27->ollama) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Install the ollama package.\n",
    "# If you encounter any issues, please refer to the ollama documentation: https://github.com/ollama/ollama-python/blob/main/README.md\n",
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docstring_parser in /Users/saurabh/miniforge3/envs/2526s1/lib/python3.10/site-packages (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "# this step is optional is you run into errors with docstring_parser\n",
    "!pip install docstring_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Initialize the ollama client\n",
    "client = ollama.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Downloading the Model\n",
    "\n",
    "**Step 2: Download a Pre‑trained Model**\n",
    "\n",
    "Generative models like `gemma3:270m` need to be downloaded before they can be used. Think of this step like downloading a textbook before you start studying. In communication science, you might compare it to gathering all the reference material before writing a research paper.\n",
    "\n",
    "We'll use the `gemma3:270m` model as our example. The `270m` means it uses 270 million parameters (which affects its performance and capabilities, something we will discuss in more detail in coming lectures). This can be downloaded from `ollama`. A list of other available models can be found [here](https://ollama.com/search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 735af2139dc6: 100% ▕██████████████████▏ 291 MB                         \u001b[K\n",
      "pulling 4b19ac7dd2fb: 100% ▕██████████████████▏  476 B                         \u001b[K\n",
      "pulling 3e2c24001f9e: 100% ▕██████████████████▏ 8.4 KB                         \u001b[K\n",
      "pulling 339e884a40f6: 100% ▕██████████████████▏   61 B                         \u001b[K\n",
      "pulling 74156d92caf6: 100% ▕██████████████████▏  490 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Pull the gemma3:270m model using ollama's CLI command\n",
    "!ollama pull gemma3:270m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Importing Ollama and Generating Text\n",
    "\n",
    "**Step 3: Import the Library and Generate Text**\n",
    "\n",
    "The ollama Python client provides a convenient interface to both access the model and generate text in one call. This is similar to opening your textbook and starting to read immediately.\n",
    "\n",
    "**Communication Science Connection:**\n",
    "\n",
    "Just as a communicator chooses the right tone for a message, using the correct model (here, `gemma3:270m`) sets the stage for how our AI will \"speak\" to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a land far away,\n",
      "A world of shimmering colors, a vibrant hue.\n",
      "The air hummed with magic, a gentle, sweet breeze,\n",
      "Whispering secrets carried on the rustling trees.\n",
      "\n",
      "The land was lush and green, a tapestry of life,\n",
      "Where rivers flowed like silver ribbons, soaring high.\n",
      "And the creatures lived in harmony, a peaceful grace,\n",
      "With furry friends and feathered creatures in their place.\n",
      "\n",
      "The mountains rose in majesty, capped with snow,\n",
      "Where eagles soared above, a silent, watchful show.\n",
      "The forests were deep and ancient, with secrets untold,\n",
      "A world of wonder, in stories yet to unfold.\n",
      "\n",
      "A crystal river flowed, a constant, steady flow,\n",
      "Reflecting the stars in its depths, a gentle, soft glow.\n",
      "And in the heart of the land, a harmony reigned,\n",
      "A world where dreams were made, and magic was always seen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a simple prompt\n",
    "prompt = \"Once upon a time in a land far away,\"\n",
    "\n",
    "# Generate text using the model with default settings\n",
    "# We use the chat API with a single user message\n",
    "result = client.chat(\n",
    "    model=\"gemma3:270m\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "print(result['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimenting with Generation Parameters\n",
    "\n",
    "**Step 4: Tweaking the Parameters**\n",
    "\n",
    "Generative models have parameters that control how text is generated — similar to how a speaker might adjust tone, speed, or emphasis when communicating.\n",
    "\n",
    "**Key Parameters:**\n",
    "- **max_tokens:** The maximum number of tokens (words, or parts of words) in the output.\n",
    "- **temperature:** Controls randomness. Lower values (e.g., 0.5) make the output more predictable, while higher values (e.g., 1.0) increase creativity (but can sometimes make the text less coherent).\n",
    "- **top_k:** Limits the options at each generation step to the top ‘k’ most likely tokens. This is like narrowing down your vocabulary to the most impactful words.\n",
    "\n",
    "**Communication Science Example:**\n",
    "In persuasive communication, carefully choosing words can alter the message's impact. A/B testing with these parameters shows how even small changes can affect the generated narrative, mirroring how slight adjustments in speech or writing style can influence audience perception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a land far away,\n",
      "Where the mountains rose in towering heights,\n",
      "And the rivers flowed with shimmering light,\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "# Generate text with tuned parameters using ollama\n",
    "response_tuned = client.chat(\n",
    "    model=\"gemma3:270m\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    options={\n",
    "        \"num_predict\": 30,    # Maximum number of tokens to generate\n",
    "        \"temperature\": 0.8,   # A balance between creativity and coherence.\n",
    "        \"top_k\": 50          # Consider only the top 50 options at each generation step.\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response_tuned['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deep Dive: Communication Concepts and AI Narratives\n",
    "\n",
    "Let's reflect on the process:\n",
    "\n",
    "- **Narrative Structure:** The prompt acts like an opening line in a story or a hook in a speech. How does the AI build on this opening?\n",
    "- **Message Framing:** The parameters (temperature, top_k) determine the \"tone\" or style. In communication, message framing can affect how a message is perceived. Similarly, the AI's output can be tuned to be more formal, creative, or conversational.\n",
    "\n",
    "**Exercise:** Experiment with different prompts (e.g., persuasive, descriptive, humorous) and adjust parameters to see how the AI's output changes. How do you think these parameters mirror techniques used by professional communicators?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Exercises and Exploration\n",
    "\n",
    "**Further Exploration:**\n",
    "\n",
    "- Try changing the prompt to something like:\n",
    "  \"In today’s rapidly changing media landscape, communication is more important than ever...\"\n",
    "  How does the AI handle modern, media-focused language compared to a fairy tale beginning?\n",
    "\n",
    "- How might understanding these AI parameters help us analyze or even craft persuasive messages in advertising, politics, or social media?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using LLM for Classification on Pandas DataFrame\n",
    "\n",
    "In this final example, we will see how to use an open‑source LLM for a simple classification task on a pandas DataFrame! Here, we have a small DataFrame with a column `film_plot` containing descriptions of film plots. We'll call the LLM to rate the uniqueness of these plots on a scale of 1–5 (with 1 being very generic, and 5 being extremely unique). This example shows how LLMs can be used for labelling tasks on structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>film_plot</th>\n",
       "      <th>uniqueness_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A group of friends embark on a quest to find a...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In a dystopian future, society is divided by c...</td>\n",
       "      <td>Rating: 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A love story unfolds between two strangers who...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A detective investigates a series of bizarre m...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An underdog sports team overcomes insurmountab...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           film_plot uniqueness_rating\n",
       "0  A group of friends embark on a quest to find a...                 5\n",
       "1  In a dystopian future, society is divided by c...         Rating: 4\n",
       "2  A love story unfolds between two strangers who...                 5\n",
       "3  A detective investigates a series of bizarre m...                 5\n",
       "4  An underdog sports team overcomes insurmountab...                 5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample pandas DataFrame with 5 film plots\n",
    "df = pd.DataFrame({\n",
    "    'film_plot': [\n",
    "        \"A group of friends embark on a quest to find a hidden treasure in the mountains.\",\n",
    "        \"In a dystopian future, society is divided by class and a young rebel fights against an oppressive regime.\",\n",
    "        \"A love story unfolds between two strangers who meet on a rainy day in a bustling city.\",\n",
    "        \"A detective investigates a series of bizarre murders on Mars that seem to be connected to an ancient curse.\",\n",
    "        \"An underdog sports team overcomes insurmountable odds to win the championship against all expectations.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Define a function to rate the uniqueness of a film plot using the LLM\n",
    "def rate_uniqueness(plot):\n",
    "    prompt = (\n",
    "        f\"Rate the uniqueness of the following film plot on a scale of 1 to 5, where 1 is very generic and 5 is extremely novel:\\n\\n\"\n",
    "        f\"{plot}\\n\\n\"\n",
    "        \"Only output the rating number.\"\n",
    "    )\n",
    "    response = client.chat(\n",
    "        model=\"gemma3:270m\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    )\n",
    "    rating = response['message']['content'].strip()\n",
    "    return rating\n",
    "\n",
    "# Apply the classification function to each film plot\n",
    "df['uniqueness_rating'] = df['film_plot'].apply(rate_uniqueness)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "**Wrapping Up:**\n",
    "\n",
    "In this notebook, we covered:\n",
    "1. How to set up your environment for working with LLMs in python using **ollama**.\n",
    "2. How to download a pre‑trained model (`gemma3:270m`) using ollama's CLI.\n",
    "3. How to generate text using simple and tuned prompts via the ollama chat API.\n",
    "4. How the parameters of generative models relate to communication techniques and message framing.\n",
    "5. How to leverage an open‑source LLM for a simple classification task on pandas data (rating film plot uniqueness).\n",
    "\n",
    "As you continue to explore and experiment, consider how these techniques can be applied responsibly to analyze or craft messages in diverse communication science domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2526s1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
